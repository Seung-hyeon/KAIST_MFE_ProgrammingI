{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4. scipy, Optimization\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/Max_paraboloid.svg/700px-Max_paraboloid.svg.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "</center>\n",
    "\n",
    "<font color='orange'>\n",
    "Seunghyeon Yu\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "* optimization\n",
    "* scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\"selection of a best element (with regard to some criterion) from some set of available alternatives.\"\n",
    "\n",
    "$$\n",
    "    \\hat{\\mathbf{x}}  = \\text{arg}\\min_{\\mathbf{x}}{f(\\mathbf{x})} \\\\\n",
    "    \\text{s.t. } \\mathbf{g}(\\mathbf{x}) = \\mathbf{0}\\\\\n",
    "    \\mathbf{h}(\\mathbf{x}) \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Subfields\n",
    "\n",
    "from the [wiki](https://en.wikipedia.org/wiki/Mathematical_optimization)\n",
    "\n",
    " * **Convex Programming** (constraint set is also convex): Easy to solve\n",
    "  * Linear Programming : use SImplex Method\n",
    "  * Quadratic : use Conjugate Gradient Method\n",
    "  * Conic Programming (General Convex Programming)\n",
    " \n",
    "and, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Nonlinear Programming** : Hard to solve\n",
    " * Grid Search, Greedy Search\n",
    " * Gradient Descent : Momentum, Adagrad, Rmsprop...\n",
    " * Nelder-Mead\n",
    " * Newton-Rhapson, BFGS, L-BFGS, SLSQP...\n",
    " * Genetic Algorithm, Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and some others...\n",
    "\n",
    "* Stochastic Programming\n",
    "* Combinatorial Optimmization\n",
    "* Infinite-dimensional Optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Programming\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}}\\mathbf{c}^T\\mathbf{x} \\\\\n",
    "    \\text{s.t. } \\mathbf{A}\\mathbf{x} - \\mathbf{b} = \\mathbf{0}\\\\\n",
    "    \\mathbf{x} \\ge \\mathbf{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Simplex Method**\n",
    "\n",
    "feasible set of $\\mathbf{x}$ is [convex polytope](https://en.wikipedia.org/wiki/Convex_polytope).\n",
    "$$\n",
    "    \\mathbf{A}\\mathbf{x} = \\mathbf{b}, \\mathbf{x} \\ge \\mathbf{0}\n",
    "$$\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d4/Simplex-method-3-dimensions.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we know that optimal point should be some vertice of the polytope.\n",
    "\n",
    "1. Choose random Vertice $\\mathbf{x_0}$ and calculate $\\mathbf{c}^T\\mathbf{x_0}$.\n",
    "2. Find the neighbor vertices $B_r(\\mathbf{x_0})$, and calculate $\\mathbf{c}^T\\mathbf{x}$ for $\\mathbf{x} \\in B_r(\\mathbf{x_0})$ each.\n",
    "3. Go to the vertice which has minimum objective function and repeat step 1 to 2 until no improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quadratic Programming\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}} \\frac{1}{2}\\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{c}^T\\mathbf{x}\\\\\n",
    "    \\text{s.t. } \\mathbf{E}\\mathbf{x} - \\mathbf{d} = \\mathbf{0}\n",
    "$$\n",
    "where $\\mathbf{Q}$ is symmetric matrix. (if not, the objective function can diverge to $-\\infty$)\n",
    "\n",
    "This problem is equivalent to solve $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ if we redefine $\\mathbf{x}$ as \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\mathbf{Q} & \\mathbf{E}^T \\\\ \\mathbf{E} & \\mathbf{0} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ \\lambda  \\end{bmatrix} = \\begin{bmatrix} -\\mathbf{c} \\\\ \\mathbf{d} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "** Conjugate Gradient**\n",
    "\n",
    "two non-zero vectors $\\mathbf{u}$ and $\\mathbf{v}$ are conjugate (with $\\mathbf{A}$) if \n",
    "$$\n",
    "    \\mathbf{u}^T\\mathbf{A}\\mathbf{v} = \\mathbf{0}\n",
    "$$\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Conjugate_gradient_illustration.svg/220px-Conjugate_gradient_illustration.svg.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we know that optimal point should be some vertice of the polytope.\n",
    "\n",
    "1. Initial guess $\\mathbf{x_0}$ and calculate $\\frac{1}{2}\\mathbf{x_0}^T\\mathbf{Q}\\mathbf{x_0} + \\mathbf{c}^T\\mathbf{x_0}$.\n",
    "2. Calculate Gradient\n",
    "$$\n",
    "    \\triangledown f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} + \\mathbf{c}\n",
    "$$\n",
    "3. Find conjugate gradient using Gram-Schmidt Orthonormalization.\n",
    "$$\n",
    "    \\mathbf{p_k} =  \\triangledown f(\\mathbf{x_k}) - \\sum_{i<k}\\frac{\\mathbf{p_i}^T\\mathbf{Q}\\triangledown f(\\mathbf{x_k})}{\\mathbf{p_i}^T\\mathbf{Q}\\mathbf{p_i}}\\mathbf{p_i}\n",
    "$$\n",
    "4. Update\n",
    "$$\n",
    "    \\mathbf{x_{k+1}} = \\mathbf{x_k} + \\frac{\\mathbf{p_k}^T\\triangledown f(\\mathbf{x_k})}{\\mathbf{p_k}^T\\mathbf{Q}\\mathbf{p_k}}\\mathbf{p_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear Programming\n",
    "$$\n",
    "    \\min_{\\mathbf{x}}{f(\\mathbf{x})} \\\\\n",
    "    \\text{s.t. } \\mathbf{g}(\\mathbf{x}) = \\mathbf{0}\\\\\n",
    "    \\mathbf{h}(\\mathbf{x}) \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grid Search\n",
    "<center>\n",
    "<img src=\"http://www.dis.uniroma1.it/~lucidi/DFL/noisy_quadratic_surf.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "\n",
    "1. make grid\n",
    "$$\n",
    "    \\mathbf{X} = [\\mathbf{x_0}, \\mathbf{x_1}, ... \\mathbf{x_N}]\n",
    "$$\n",
    "2. find the best point from $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "$\n",
    "    \\mathbf{x_{k+1}} = \\mathbf{update}(\\mathbf{x_k})\n",
    "$\n",
    "\n",
    "<center>\n",
    "<img src=\"http://i.imgur.com/2dKCQHh.gif?1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"http://i.imgur.com/NKsFHJb.gif?1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nelder-Mead method\n",
    "\n",
    "Also know as downhill simplex method\n",
    "\n",
    "* **Reflection** : reflect worst vertice, and check objective function better.\n",
    "* **Shrink** : if above procedure makes no progress, shrink the simplex.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/320px-Nelder-Mead_Himmelblau.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Newton's method\n",
    "also known as the Newton-Raphson method, is finding $\\triangledown f(\\mathbf{x})=0$.\n",
    "$$\n",
    "    \\mathbf{x_{k+1}} = \\mathbf{x_k} - \\frac{\\triangledown f(\\mathbf{x_k})}{\\triangledown^2 f(\\mathbf{x_k})}\n",
    "$$\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BFGS method\n",
    "Broyden-Fletcher-Goldfarb-Shannon algorithm\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/de/f/ff/Rosenbrock-bfgs-animation.gif\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Inital guess $\\mathbf{x_0}$ and an approximate Hessian $\\mathbf{B_0}$.\n",
    "1. Obtain a direction $\\mathbf{p_k}$ by solving $\\mathbf{B_k}\\mathbf{p_k} = -\\triangledown f(\\mathbf{x_k})$.\n",
    "1. Perform a line search to find stepsize $\\alpha_k$ in the direction found: $\\alpha_k = arg \\min{f(\\mathbf{x_k} + \\alpha \\mathbf{p_k})}$.\n",
    "1. Set $\\mathbf{s_k} = \\alpha_k \\mathbf{p_k}$ and update $\\mathbf{x_{k+1}} = \\mathbf{x_k} + \\mathbf{s_k}$.\n",
    "1. $\\mathbf{y_k} = \\triangledown f(\\mathbf{x_{k+1}}) - \\triangledown f(\\mathbf{x_k})$.\n",
    "1. $$\\mathbf{B_{k+1}} = \\mathbf{B_k} + \\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} - \\frac{\\mathbf{B_k} \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B_k} }{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B_k }\\mathbf{s}_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scipy\n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.scipy.org/doc/scipy-0.7.x/reference/_static/scipyshiny_small.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functions in Scipy\n",
    "\n",
    "* [Special functions](https://docs.scipy.org/doc/scipy/reference/tutorial/special.html) : Bessel, Gamma, Hankel...\n",
    "* [Integration](https://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html) : quad, trapz, romb...\n",
    "* [Interpolation](https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html) : Spline\n",
    "* [Fourier Transform](https://docs.scipy.org/doc/scipy/reference/tutorial/fftpack.html) : FFT, cosine, sine transform...\n",
    "* [Sparse Matrix](https://docs.scipy.org/doc/scipy/reference/tutorial/csgraph.html) : csr, csg...\n",
    "* [Statistics](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html) : CDF, PDF, Moments...\n",
    "* [Optimization](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    \\min{f(\\mathbf{x}) = \\| \\mathbf{x} \\|^2_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 645\n",
      "         Function evaluations: 1006\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,1,3,1,4,2,1])\n",
    "res = minimize(f, x0, method='nelder-mead', \n",
    "               options={'xtol':1e-8, 'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51295670e-09, -5.48393611e-09, -3.15724851e-09, -3.13295585e-10,\n",
       "       -1.85182626e-09, -2.38157410e-09, -8.95263481e-10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Optimmization ** with **Jacobian** $\\triangledown f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 4\n",
      "         Gradient evaluations: 4\n"
     ]
    }
   ],
   "source": [
    "def del_f(x):\n",
    "    return 2*x\n",
    "\n",
    "res = minimize(f, x0, method='BFGS', jac=del_f,\n",
    "               options={'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.33066907e-16, -1.66533454e-16,  1.55431223e-15,  1.11022302e-16,\n",
       "       -4.44089210e-16,  3.33066907e-16,  5.55111512e-17])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Optimmization ** with **Jacobian** $\\triangledown f(\\mathbf{x})$, **Hessian** $\\triangledown^2 f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 2\n",
      "         Function evaluations: 3\n",
      "         Gradient evaluations: 4\n",
      "         Hessian evaluations: 2\n"
     ]
    }
   ],
   "source": [
    "def del_sq_f(x):\n",
    "    return 2*np.eye(len(x))\n",
    "\n",
    "res = minimize(f, x0, method='Newton-CG',\n",
    "               jac=del_f, hess=del_sq_f,\n",
    "               options={'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Optimization** with **Constraints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "    \\min{f(\\mathbf{x}) = \\| \\mathbf{x} \\|^2_2} \\\\\n",
    "    \\text{s.t. } x_1 + x_2 = 3 \\\\\n",
    "    x_3 + 2 \\ge 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "constraints = [{'type':'eq', 'fun':lambda x:x[0] + x[1] -3},\n",
    "               {'type':'ineq', 'fun':lambda x:x[2] - 2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 8.50000000000001\n",
      "            Iterations: 5\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "res = minimize(f, x0, jac=del_f, method='SLSQP',\n",
    "               constraints=constraints, options={'disp':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** [EX 1] Ordinary Least Square **\n",
    "$$\n",
    "    \\hat{\\mathbf{\\beta}}_{OLS} = arg \\min_{\\mathbf{\\beta}}{\\| \\mathbf{y} - \\mathbf{X}\\mathbf{\\beta} \\|^2_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 1],\n",
    "              [3, 1, 2],\n",
    "              [1, 2, 3],\n",
    "              [1, 1, 5]])\n",
    "y = np.array([2, 4, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3381295 ,  0.67625899, -0.36450839])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numerical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(b, X, y):\n",
    "    return np.linalg.norm(y-X.dot(b))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.306954\n",
      "         Iterations: 212\n",
      "         Function evaluations: 380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.33812949,  0.67625899, -0.36450839])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0 = np.random.randn(X.shape[1])\n",
    "minimize(f, b0, args=(X, y,), method='nelder-mead', \n",
    "               options={'xtol':1e-8, 'disp':True}).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** [EX 2] Lasso **\n",
    "\n",
    "(least absolute shrinkage and selection operator)\n",
    "$$\n",
    "    \\hat{\\mathbf{\\beta}}_{Lasso} = arg \\min_{\\mathbf{\\beta}}{\\frac{1}{N}\\| \\mathbf{y} - \\mathbf{X}\\mathbf{\\beta} \\|^2_2 + \\lambda \\|\\beta\\|_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analytical Solution \n",
    "\n",
    "$$\n",
    "    \\hat{\\mathbf{\\beta}}_{Lasso, i} = \\text{sgn}(\\hat{\\beta}_{OLS, i})[|\\hat{\\beta}_{OLS, i}| - \\lambda ]_+\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numerical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def f(b, X, y):\n",
    "    return (np.linalg.norm(y-X.dot(b))**2)/len(y) + 1*np.linalg.norm(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.892086\n",
      "         Iterations: 178\n",
      "         Function evaluations: 328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.07913669,  0.15827338, -0.01438849])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0 = np.random.randn(X.shape[1])\n",
    "minimize(f, b0, args=(X, y,), method='nelder-mead', \n",
    "               options={'xtol':1e-8, 'disp':True}).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other useful packages\n",
    "\n",
    "* [statsmodels](http://www.statsmodels.org/stable/index.html)\n",
    "* [seaborn](https://seaborn.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
